{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5739234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # For transformers v3.x: \n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "# line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "\n",
    "# input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = phobert(input_ids)  # Models outputs are now tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09866b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d94835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  218,    8,  649,  212,  956, 2413,    5,    2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import (WEIGHTS_NAME, AdamW, BertConfig,\n",
    "                                  BertForTokenClassification, BertTokenizer,\n",
    "                                  WarmupLinearSchedule)\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Ner(BertForTokenClassification):\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,valid_ids=None,attention_mask_label=None):\n",
    "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask,head_mask=None)[0]\n",
    "        batch_size,max_len,feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda')\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                    if valid_ids[i][j].item() == 1:\n",
    "                        jj += 1\n",
    "                        valid_output[i][jj] = sequence_output[i][j]\n",
    "        sequence_output = self.dropout(valid_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            # Only keep active parts of the loss\n",
    "            #attention_mask_label = None\n",
    "            if attention_mask_label is not None:\n",
    "                active_loss = attention_mask_label.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "\n",
    "def readfile(filename):\n",
    "    '''\n",
    "    read file\n",
    "    '''\n",
    "    f = open(filename)\n",
    "    data = []\n",
    "    sentence = []\n",
    "    label= []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence,label))\n",
    "                sentence = []\n",
    "                label = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) >0:\n",
    "        data.append((sentence,label))\n",
    "        sentence = []\n",
    "        label = []\n",
    "    return data\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        return readfile(input_file)\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoNLL-2003 data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"valid.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "    def _create_examples(self,lines,set_type):\n",
    "        examples = []\n",
    "        for i,(sentence,label) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = ' '.join(sentence)\n",
    "            text_b = None\n",
    "            label = label\n",
    "            examples.append(InputExample(guid=guid,text_a=text_a,text_b=text_b,label=label))\n",
    "        return examples\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list,1)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index,example) in enumerate(examples):\n",
    "        textlist = example.text_a.split(' ')\n",
    "        labellist = example.label\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        for i, word in enumerate(textlist):\n",
    "            token = tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            label_1 = labellist[i]\n",
    "            for m in range(len(token)):\n",
    "                if m == 0:\n",
    "                    labels.append(label_1)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                else:\n",
    "                    valid.append(0)\n",
    "        if len(tokens) >= max_seq_length - 1:\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "            labels = labels[0:(max_seq_length - 2)]\n",
    "            valid = valid[0:(max_seq_length - 2)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
    "        ntokens = []\n",
    "        segment_ids = []\n",
    "        label_ids = []\n",
    "        ntokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.insert(0,1)\n",
    "        label_mask.insert(0,1)\n",
    "        label_ids.append(label_map[\"[CLS]\"])\n",
    "        for i, token in enumerate(tokens):\n",
    "            ntokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.append(1)\n",
    "        label_mask.append(1)\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        label_mask = [1] * len(label_ids)\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            valid.append(1)\n",
    "            label_mask.append(0)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            # logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_ids,\n",
    "                              valid_ids=valid,\n",
    "                              label_mask=label_mask))\n",
    "    return features\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Required parameters\n",
    "    parser.add_argument(\"--data_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "    parser.add_argument(\"--task_name\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The name of the task to train.\")\n",
    "    parser.add_argument(\"--output_dir\",\n",
    "                        default=None,\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"\",\n",
    "                        type=str,\n",
    "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "    parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--do_train\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval or not.\")\n",
    "    parser.add_argument(\"--eval_on\",\n",
    "                        default=\"dev\",\n",
    "                        help=\"Whether to run eval on the dev set or test set.\")\n",
    "    parser.add_argument(\"--do_lower_case\",\n",
    "                        action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "    parser.add_argument(\"--train_batch_size\",\n",
    "                        default=32,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        default=5e-5,\n",
    "                        type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_train_epochs\",\n",
    "                        default=3.0,\n",
    "                        type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.01, type=float,\n",
    "                        help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                        help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                        help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--no_cuda\",\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "    parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help=\"local_rank for distributed training on gpus\")\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument('--gradient_accumulation_steps',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
    "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "    parser.add_argument('--loss_scale',\n",
    "                        type=float, default=0,\n",
    "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
    "    parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "    parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    processors = {\"ner\":NerProcessor}\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if not args.do_train and not args.do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    task_name = args.task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list) + 1\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    train_examples = None\n",
    "    num_train_optimization_steps = 0\n",
    "    if args.do_train:\n",
    "        train_examples = processor.get_train_examples(args.data_dir)\n",
    "        num_train_optimization_steps = int(\n",
    "            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "        if args.local_rank != -1:\n",
    "            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    # Prepare model\n",
    "    config = BertConfig.from_pretrained(args.bert_model, num_labels=num_labels, finetuning_task=args.task_name)\n",
    "    model = Ner.from_pretrained(args.bert_model,\n",
    "              from_tf = False,\n",
    "              config = config)\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias','LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    warmup_steps = int(args.warmup_proportion * num_train_optimization_steps)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=num_train_optimization_steps)\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    global_step = 0\n",
    "    nb_tr_steps = 0\n",
    "    tr_loss = 0\n",
    "    label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "    if args.do_train:\n",
    "        train_features = convert_examples_to_features(\n",
    "            train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "        all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n",
    "        all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(train_data)\n",
    "        else:\n",
    "            train_sampler = DistributedSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "        model.train()\n",
    "        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\n",
    "                loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\n",
    "                if n_gpu > 1:\n",
    "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "                if args.fp16:\n",
    "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "        # Save a trained model and the associated configuration\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "        model_config = {\"bert_model\":args.bert_model,\"do_lower\":args.do_lower_case,\"max_seq_length\":args.max_seq_length,\"num_labels\":len(label_list)+1,\"label_map\":label_map}\n",
    "        json.dump(model_config,open(os.path.join(args.output_dir,\"model_config.json\"),\"w\"))\n",
    "        # Load a trained model and config that you have fine-tuned\n",
    "    else:\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = Ner.from_pretrained(args.output_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        if args.eval_on == \"dev\":\n",
    "            eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "        elif args.eval_on == \"test\":\n",
    "            eval_examples = processor.get_test_examples(args.data_dir)\n",
    "        else:\n",
    "            raise ValueError(\"eval on dev or test set only\")\n",
    "        eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        all_valid_ids = torch.tensor([f.valid_ids for f in eval_features], dtype=torch.long)\n",
    "        all_lmask_ids = torch.tensor([f.label_mask for f in eval_features], dtype=torch.long)\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "        for input_ids, input_mask, segment_ids, label_ids,valid_ids,l_mask in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            valid_ids = valid_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            l_mask = l_mask.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask,valid_ids=valid_ids,attention_mask_label=l_mask)\n",
    "\n",
    "            logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            input_mask = input_mask.to('cpu').numpy()\n",
    "\n",
    "            for i, label in enumerate(label_ids):\n",
    "                temp_1 = []\n",
    "                temp_2 = []\n",
    "                for j,m in enumerate(label):\n",
    "                    if j == 0:\n",
    "                        continue\n",
    "                    elif label_ids[i][j] == len(label_map):\n",
    "                        y_true.append(temp_1)\n",
    "                        y_pred.append(temp_2)\n",
    "                        break\n",
    "                    else:\n",
    "                        temp_1.append(label_map[label_ids[i][j]])\n",
    "                        temp_2.append(label_map[logits[i][j]])\n",
    "\n",
    "        report = classification_report(y_true, y_pred,digits=4)\n",
    "        logger.info(\"\\n%s\", report)\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            logger.info(\"\\n%s\", report)\n",
    "            writer.write(report)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_ner.py --data_dir=data/ --bert_model=bert-base-cased --task_name=ner --output_dir=out_base --max_seq_length=128 --do_train --num_train_epochs 5 --do_eval --warmup_proportion=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let’s define some variables that we need for further pre-processing steps and training the model:\n",
    "\n",
    "MAX_LENGTH = 120 #@param {type: \"integer\"}\n",
    "MODEL = \"chriskhanhtran/spanberta\" #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\"]\n",
    "\n",
    "The script below will split sentences longer than MAX_LENGTH (in terms of tokens) into small ones. Otherwise, long sentences will be truncated when tokenized, causing the loss of training data and some tokens in the test set not being predicted.\n",
    "\n",
    "%%capture\n",
    "!wget \"https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\"\n",
    "\n",
    "!python3 preprocess.py train_temp.txt $MODEL $MAX_LENGTH > train.txt\n",
    "!python3 preprocess.py dev_temp.txt $MODEL $MAX_LENGTH > dev.txt\n",
    "!python3 preprocess.py test_temp.txt $MODEL $MAX_LENGTH > test.txt\n",
    "\n",
    "# If your dataset has different labels or more labels than CoNLL-2002/2003 datasets, run the line below to get unique labels from your data and save them into labels.txt. This file will be used when we start fine-tuning our model.\n",
    "\n",
    "!cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n",
    "\n",
    "\n",
    "# training hyperparameters\n",
    "MAX_LENGTH = 128 #@param {type: \"integer\"}\n",
    "MODEL = \"chriskhanhtran/spanberta\" #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\"]\n",
    "OUTPUT_DIR = \"spanberta-ner\" #@param [\"spanberta-ner\", \"bert-base-ml-ner\"]\n",
    "BATCH_SIZE = 32 #@param {type: \"integer\"}\n",
    "NUM_EPOCHS = 3 #@param {type: \"integer\"}\n",
    "SAVE_STEPS = 100 #@param {type: \"integer\"}\n",
    "LOGGING_STEPS = 100 #@param {type: \"integer\"}\n",
    "SEED = 42 #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "!python3 run_ner.py \\\n",
    "  --data_dir ./ \\\n",
    "  --model_type bert \\\n",
    "  --labels ./labels.txt \\\n",
    "  --model_name_or_path $MODEL \\\n",
    "  --output_dir $OUTPUT_DIR \\\n",
    "  --max_seq_length  $MAX_LENGTH \\\n",
    "  --num_train_epochs $NUM_EPOCHS \\\n",
    "  --per_gpu_train_batch_size $BATCH_SIZE \\\n",
    "  --save_steps $SAVE_STEPS \\\n",
    "  --logging_steps $LOGGING_STEPS \\\n",
    "  --seed $SEED \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_predict \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10825e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_ner.py --data_dir=data/vlsp16 --bert_model='/home/phamson/Desktop/phobert-base-135' --task_name=ner --output_dir=out_base --max_seq_length=128 --do_train --num_train_epochs 5 --do_eval --warmup_proportion=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0185853",
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_ner.py \\\n",
    "  --model_name_or_path vinai/phobert-base \\\n",
    "  --dataset_name '/home/phamson/transformers/examples/token-classification/vlsp16' \\\n",
    "  --output_dir '/home/phamson/ner-phobert' \\\n",
    "  --do_train \\\n",
    "  --do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb5995d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a396e420bd2e0c4c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/phamson/.cache/huggingface/datasets/csv/default-a396e420bd2e0c4c/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/phamson/transformers/examples/token-classification/vlsp16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af119bb969f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/phamson/transformers/examples/token-classification/vlsp16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF google storage unreachable. Downloading and preparing it from source\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdownloaded_from_gcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                         self._download_and_prepare(\n\u001b[0m\u001b[1;32m    579\u001b[0m                             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                         )\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0msplit_generators_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_split_generators_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0msplit_generators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msplit_generators_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# Checksums verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/packaged_modules/csv/csv.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"At least one data file must be specified, but got data_files={self.config.data_files}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mextracted_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_recorded_sizes_checksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_sizes_checksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownloaded_path_or_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checksum Computation took {} min\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/download_manager.py\u001b[0m in \u001b[0;36m_record_sizes_checksums\u001b[0;34m(self, url_or_urls, downloaded_path_or_paths)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownloaded_path_or_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# call str to support PathLike objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recorded_sizes_checksums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_size_checksum_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/info_utils.py\u001b[0m in \u001b[0;36mget_size_checksum_dict\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m\"\"\"Compute the file size and the sha256 checksum of a file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/phamson/transformers/examples/token-classification/vlsp16'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files = '/home/phamson/transformers/examples/token-classification/vlsp16', delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be620ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a6e7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e343e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlsp_train = '/home/phamson/Downloads/VLSP2016-NER/train.txt'\n",
    "vlsp_test = '/home/phamson/Downloads/VLSP2016-NER/test.txt'\n",
    "vlsp_dev = '/home/phamson/Downloads/VLSP2016-NER/dev.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f57d90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_train = '/home/phamson/Downloads/VLSP2016-NER/train_ner.csv'\n",
    "v_test = '/home/phamson/Downloads/VLSP2016-NER/test_ner.csv'\n",
    "v_dev = '/home/phamson/Downloads/VLSP2016-NER/dev_ner.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "467f600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(vlsp_train, delimiter = '\\t', header = None, quotechar=\"'\")\n",
    "test = pd.read_csv(vlsp_test, delimiter = '\\t', header = None, quotechar=\"'\")\n",
    "dev = pd.read_csv(vlsp_dev, delimiter = '\\t', header = None, quotechar=\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df07f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(vlsp, delimiter = '\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "775a636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vlsp_ner = df.drop(df.columns[[2,1]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfb86757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/home/phamson/Downloads/VLSP2016-NER/train_ner.txt', sep='\\t', columns=[0,3], header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "343de5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('/home/phamson/Downloads/VLSP2016-NER/train_ner.csv', sep='\\t', columns=[0,3], header=False, index=False)\n",
    "test.to_csv('/home/phamson/Downloads/VLSP2016-NER/test_ner.csv', sep='\\t', columns=[0,3], header=False, index=False)\n",
    "dev.to_csv('/home/phamson/Downloads/VLSP2016-NER/dev_ner.csv', sep='\\t', columns=[0,3], header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7408ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/phamson/Downloads/VLSP2016-NER/train.txt') as file:\n",
    "    csv_reader = csv.reader(file, delimiter='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(vlsp_train,\"rb\") as source:\n",
    "    rdr= csv.reader( source )\n",
    "    with open(v_train,\"wb\") as result:\n",
    "        wtr= csv.writer( result )\n",
    "        for r in rdr:\n",
    "            wtr.writerow( (r[0], r[1], r[3], r[4]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'ner'\n",
    "model_checkpoint = ''\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75ee8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/phamson/jupyter-notebook/run_ner.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python run_ner.py \\\n",
    "  --model_name_or_path vinai/phobert-base \\\n",
    "  --train_file /home/phamson/Downloads/VLSP2016-NER/ner/train.csv \\\n",
    "  --validation_file /home/phamson/Downloads/VLSP2016-NER/ner/dev.csv \\\n",
    "  --output_dir /output \\\n",
    "  --do_train \\\n",
    "  --do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e176b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_conll(text, sep=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Converts data in CoNLL format to word and label lists.\n",
    "    Args:\n",
    "        text (str): Text string in conll format, e.g.\n",
    "            \"Amy B-PER\n",
    "             ADAMS I-PER\n",
    "             works O\n",
    "             at O\n",
    "             the O\n",
    "             University B-ORG\n",
    "             of I-ORG\n",
    "             Minnesota I-ORG\n",
    "             . O\"\n",
    "        sep (str, optional): Column separator\n",
    "            Defaults to \\t\n",
    "    Returns:\n",
    "        tuple:\n",
    "            (list of word lists, list of token label lists)\n",
    "    \"\"\"\n",
    "    text_list = text.split(\"\\n\\n\")\n",
    "    if text_list[-1] in (\" \", \"\"):\n",
    "        text_list = text_list[:-1]\n",
    "\n",
    "    max_seq_len = 0\n",
    "    sentence_list = []\n",
    "    labels_list = []\n",
    "    for s in text_list:\n",
    "        # split each sentence string into \"word label\" pairs\n",
    "        s_split = s.split(\"\\n\")\n",
    "        # split \"word label\" pairs\n",
    "        s_split_split = [t.split(sep) for t in s_split]\n",
    "        sentence_list.append([t[0] for t in s_split_split if len(t) > 1])\n",
    "        labels_list.append([t[1] for t in s_split_split if len(t) > 1])\n",
    "        if len(s_split_split) > max_seq_len:\n",
    "            max_seq_len = len(s_split_split)\n",
    "    print(\"Maximum sequence length is: {0}\".format(max_seq_len))\n",
    "    return sentence_list, labels_list\n",
    "\n",
    "\n",
    "def read_conll_file(file_path, sep=\"\\t\", encoding=None):\n",
    "    \"\"\"\n",
    "    Reads a data file in CoNLL format and returns word and label lists.\n",
    "    Args:\n",
    "        file_path (str): Data file path.\n",
    "        sep (str, optional): Column separator. Defaults to \"\\t\".\n",
    "        encoding (str): File encoding used when reading the file.\n",
    "            Defaults to None.\n",
    "    Returns:\n",
    "        (list, list): A tuple of word and label lists (list of lists).\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding=encoding) as f:\n",
    "        data = f.read()\n",
    "    return preprocess_conll(data, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8754d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length is: 45706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['Người',\n",
       "   'cầm',\n",
       "   'thư',\n",
       "   'đã',\n",
       "   'chết',\n",
       "   'còn',\n",
       "   'người',\n",
       "   'nhận',\n",
       "   'thư',\n",
       "   'thì',\n",
       "   'bị',\n",
       "   'bắt',\n",
       "   '!',\n",
       "   '',\n",
       "   'Biết_bao_nhiêu',\n",
       "   'bà',\n",
       "   'mẹ',\n",
       "   'như',\n",
       "   'mẹ',\n",
       "   'Đường',\n",
       "   'sẽ',\n",
       "   'còn',\n",
       "   'đau_khổ',\n",
       "   'khóc_than',\n",
       "   'đến',\n",
       "   'cạn',\n",
       "   'dòng',\n",
       "   'nước_mắt',\n",
       "   '.',\n",
       "   '',\n",
       "   'Ôi',\n",
       "   'nếu',\n",
       "   'mình',\n",
       "   'ngã',\n",
       "   'xuống',\n",
       "   ',',\n",
       "   'mẹ',\n",
       "   'mình',\n",
       "   'cũng',\n",
       "   'sẽ',\n",
       "   'như',\n",
       "   'bà',\n",
       "   'mẹ',\n",
       "   'ấy',\n",
       "   'thôi',\n",
       "   ',',\n",
       "   'cũng',\n",
       "   'sẽ',\n",
       "   'là',\n",
       "   'một',\n",
       "   'bà',\n",
       "   'mẹ',\n",
       "   'suốt',\n",
       "   'đời',\n",
       "   'hi_sinh',\n",
       "   'vì',\n",
       "   'con',\n",
       "   'để',\n",
       "   'rồi',\n",
       "   'mãi_mãi',\n",
       "   'đau_xót',\n",
       "   'vì',\n",
       "   'con',\n",
       "   'mình',\n",
       "   'đã',\n",
       "   'ngã',\n",
       "   'xuống',\n",
       "   'nơi',\n",
       "   'chiến_trường',\n",
       "   'khói_lửa',\n",
       "   '.',\n",
       "   '',\n",
       "   'Mẹ',\n",
       "   'ơi',\n",
       "   '!',\n",
       "   '',\n",
       "   'Con',\n",
       "   'biết',\n",
       "   'nói',\n",
       "   'sao',\n",
       "   'khi',\n",
       "   'lòng',\n",
       "   'con',\n",
       "   'thương',\n",
       "   'mẹ',\n",
       "   'trăm',\n",
       "   'nghìn',\n",
       "   'triệu',\n",
       "   'mà',\n",
       "   'cũng',\n",
       "   'đành',\n",
       "   'xa',\n",
       "   'mẹ',\n",
       "   'ra',\n",
       "   'đi',\n",
       "   '.',\n",
       "   '',\n",
       "   'Quân_thù',\n",
       "   'đang',\n",
       "   'còn',\n",
       "   'đó',\n",
       "   ',',\n",
       "   'bao_nhiêu',\n",
       "   'bà',\n",
       "   'mẹ',\n",
       "   'còn',\n",
       "   'mất',\n",
       "   'con',\n",
       "   ',',\n",
       "   'bao_nhiêu',\n",
       "   'người',\n",
       "   'chồng',\n",
       "   'mất',\n",
       "   'vợ',\n",
       "   '.',\n",
       "   '',\n",
       "   'Đau_xót',\n",
       "   'vô_cùng',\n",
       "   '.',\n",
       "   '',\n",
       "   '20.5.68',\n",
       "   '',\n",
       "   'Tiễn_chân',\n",
       "   'những',\n",
       "   'bệnh_nhân',\n",
       "   'lên_đường',\n",
       "   'trở_về',\n",
       "   'đội_ngũ',\n",
       "   'chiến_đấu',\n",
       "   ',',\n",
       "   'lẽ_ra',\n",
       "   'chỉ',\n",
       "   'là',\n",
       "   'niềm',\n",
       "   'vui',\n",
       "   ',',\n",
       "   'vậy_mà',\n",
       "   'cả',\n",
       "   'người',\n",
       "   'đi',\n",
       "   'lẫn',\n",
       "   'người',\n",
       "   'ở',\n",
       "   'đều',\n",
       "   'buồn',\n",
       "   'thấm_thía',\n",
       "   '.',\n",
       "   '',\n",
       "   'Hơn',\n",
       "   'một',\n",
       "   'tháng',\n",
       "   'nằm',\n",
       "   'lại',\n",
       "   'bệnh_xá',\n",
       "   ',',\n",
       "   'những',\n",
       "   'bệnh_nhân',\n",
       "   'ấy',\n",
       "   'đã',\n",
       "   'gắn_bó',\n",
       "   'với',\n",
       "   'mình',\n",
       "   'không',\n",
       "   'phải',\n",
       "   'chỉ',\n",
       "   'là',\n",
       "   'tình_thương',\n",
       "   'giữa',\n",
       "   'người',\n",
       "   'thầy_thuốc',\n",
       "   'với',\n",
       "   'bệnh_nhân',\n",
       "   'mà',\n",
       "   'trong',\n",
       "   'tình_cảm',\n",
       "   'ấy',\n",
       "   'có',\n",
       "   'cả',\n",
       "   'nỗi',\n",
       "   'cảm_thông',\n",
       "   'sâu_sắc',\n",
       "   'giữa',\n",
       "   'những',\n",
       "   'người',\n",
       "   'bạn',\n",
       "   '.',\n",
       "   '',\n",
       "   'Hôm_nay',\n",
       "   'họ',\n",
       "   'đi',\n",
       "   'rồi',\n",
       "   ',',\n",
       "   'người',\n",
       "   'ra',\n",
       "   'đi',\n",
       "   'còn',\n",
       "   'nhớ',\n",
       "   'chăng',\n",
       "   'những',\n",
       "   'đêm',\n",
       "   'dài',\n",
       "   'trò_chuyện',\n",
       "   'những',\n",
       "   'đêm',\n",
       "   'mình',\n",
       "   'đi',\n",
       "   'trực',\n",
       "   '.',\n",
       "   '',\n",
       "   'Nhớ',\n",
       "   'chăng',\n",
       "   'những',\n",
       "   'buổi',\n",
       "   'cả',\n",
       "   'cơ_quan',\n",
       "   'đi',\n",
       "   'cõng',\n",
       "   'gạo',\n",
       "   'họ',\n",
       "   'đã',\n",
       "   'cùng',\n",
       "   'mình',\n",
       "   'xử_trí',\n",
       "   'một',\n",
       "   'cas',\n",
       "   'thương',\n",
       "   ',',\n",
       "   'họ',\n",
       "   'làm',\n",
       "   'như',\n",
       "   'những',\n",
       "   'nhân_viên',\n",
       "   'thực_thụ',\n",
       "   ',',\n",
       "   'đêm',\n",
       "   'đến',\n",
       "   'trong',\n",
       "   'ánh',\n",
       "   'đèn',\n",
       "   'dầu',\n",
       "   'họ',\n",
       "   'ngồi',\n",
       "   'hí_hoáy',\n",
       "   'lau',\n",
       "   'dụng_cụ',\n",
       "   '...',\n",
       "   '',\n",
       "   'Những',\n",
       "   'ngày',\n",
       "   'ấy',\n",
       "   'vui',\n",
       "   'sao',\n",
       "   '!',\n",
       "   '',\n",
       "   'Bao_giờ',\n",
       "   'gặp',\n",
       "   'lại',\n",
       "   'nhau',\n",
       "   'và',\n",
       "   'có',\n",
       "   'còn',\n",
       "   'được',\n",
       "   'gặp',\n",
       "   'nhau',\n",
       "   'không',\n",
       "   'hở',\n",
       "   'những',\n",
       "   'người',\n",
       "   'bạn',\n",
       "   'mến_thương',\n",
       "   '?',\n",
       "   '',\n",
       "   '20.7.68',\n",
       "   '',\n",
       "   'Những',\n",
       "   'ngày',\n",
       "   'bận_rộn',\n",
       "   'công_tác',\n",
       "   'dồn_dập',\n",
       "   ',',\n",
       "   'thương',\n",
       "   'nặng',\n",
       "   ',',\n",
       "   'người',\n",
       "   'ít',\n",
       "   ',',\n",
       "   'mọi',\n",
       "   'người',\n",
       "   'trong',\n",
       "   'bệnh_xá',\n",
       "   'đều',\n",
       "   'hết_sức',\n",
       "   'vất_vả',\n",
       "   '.',\n",
       "   '',\n",
       "   'Riêng',\n",
       "   'mình',\n",
       "   'trách_nhiệm',\n",
       "   'càng',\n",
       "   'nặng_nề',\n",
       "   'hơn',\n",
       "   'bao_giờ',\n",
       "   'hết',\n",
       "   ',',\n",
       "   'mỗi',\n",
       "   'ngày',\n",
       "   'làm_việc',\n",
       "   'từ',\n",
       "   'sáng',\n",
       "   'tinh_mơ',\n",
       "   'cho',\n",
       "   'đến',\n",
       "   'đêm',\n",
       "   'khuya',\n",
       "   '.',\n",
       "   '',\n",
       "   'Khối_lượng',\n",
       "   'công_việc',\n",
       "   'quá',\n",
       "   'lớn',\n",
       "   'mà',\n",
       "   'người',\n",
       "   'không',\n",
       "   'có',\n",
       "   'nên',\n",
       "   'một_mình',\n",
       "   'mình',\n",
       "   'vừa',\n",
       "   'phụ_trách',\n",
       "   'bệnh_xá',\n",
       "   ',',\n",
       "   'vừa',\n",
       "   'lo',\n",
       "   'điều_trị',\n",
       "   ',',\n",
       "   'vừa',\n",
       "   'giảng_dạy',\n",
       "   '.',\n",
       "   '',\n",
       "   'Vô_cùng',\n",
       "   'vất_vả',\n",
       "   'và',\n",
       "   'cũng',\n",
       "   'còn',\n",
       "   'nhiều',\n",
       "   'khó_khăn',\n",
       "   'trong',\n",
       "   'công_việc',\n",
       "   ',',\n",
       "   'nhưng',\n",
       "   'hơn',\n",
       "   'bao_giờ',\n",
       "   'hết',\n",
       "   'mình',\n",
       "   'cảm_thấy',\n",
       "   'rằng',\n",
       "   'mình',\n",
       "   'đã',\n",
       "   'đem',\n",
       "   'hết',\n",
       "   'tài_năng',\n",
       "   'sức_lực',\n",
       "   'của',\n",
       "   'mình',\n",
       "   'để',\n",
       "   'cống_hiến',\n",
       "   'cho',\n",
       "   'cách_mạng',\n",
       "   '.',\n",
       "   '',\n",
       "   'Đôi',\n",
       "   'mắt',\n",
       "   'người',\n",
       "   'thương_binh',\n",
       "   'hôm',\n",
       "   'nào',\n",
       "   'đau_nhức',\n",
       "   'tưởng',\n",
       "   'như',\n",
       "   'bỏ',\n",
       "   'hôm_nay',\n",
       "   'cũng',\n",
       "   'đã',\n",
       "   'sáng',\n",
       "   'lại',\n",
       "   'một',\n",
       "   'phần',\n",
       "   '.',\n",
       "   '',\n",
       "   'Cánh_tay',\n",
       "   'anh',\n",
       "   'bộ_đội',\n",
       "   'sưng',\n",
       "   'phù',\n",
       "   'đe_doạ',\n",
       "   'chảy',\n",
       "   'máu',\n",
       "   'bây_giờ',\n",
       "   'cũng',\n",
       "   'đã',\n",
       "   'lành_lặn',\n",
       "   '.',\n",
       "   '',\n",
       "   'Những',\n",
       "   'cánh_tay',\n",
       "   'xương',\n",
       "   'gãy',\n",
       "   'rời',\n",
       "   'cũng',\n",
       "   'đã',\n",
       "   'liền',\n",
       "   'lại',\n",
       "   '...',\n",
       "   '',\n",
       "   'Đó',\n",
       "   'chính',\n",
       "   'là',\n",
       "   'nhờ',\n",
       "   'sức_lực',\n",
       "   'của',\n",
       "   'mình',\n",
       "   'và',\n",
       "   'những',\n",
       "   'người',\n",
       "   'y_tá',\n",
       "   'đêm_ngày',\n",
       "   'lăn_lộn',\n",
       "   'trong',\n",
       "   'công_tác',\n",
       "   'bên',\n",
       "   'giường_bệnh',\n",
       "   '.',\n",
       "   '',\n",
       "   'Và',\n",
       "   'với',\n",
       "   'những',\n",
       "   'học_sinh',\n",
       "   ',',\n",
       "   'mình',\n",
       "   'cũng',\n",
       "   'đã',\n",
       "   'đem',\n",
       "   'lại',\n",
       "   'những',\n",
       "   'điều',\n",
       "   'quí_giá',\n",
       "   'trong',\n",
       "   'lý_luận',\n",
       "   'về',\n",
       "   'y_học',\n",
       "   '.',\n",
       "   '',\n",
       "   'Mình',\n",
       "   'đến',\n",
       "   'với',\n",
       "   'lớp',\n",
       "   'không',\n",
       "   'phải',\n",
       "   'chỉ',\n",
       "   'vì',\n",
       "   'tinh_thần',\n",
       "   'trách_nhiệm',\n",
       "   'mà',\n",
       "   'bằng',\n",
       "   'cả',\n",
       "   'tình_thương',\n",
       "   'của',\n",
       "   'một',\n",
       "   'người',\n",
       "   'chị',\n",
       "   'đối_với',\n",
       "   'những',\n",
       "   'đứa',\n",
       "   'em',\n",
       "   'đã',\n",
       "   'chịu',\n",
       "   'biết_bao',\n",
       "   'thiệt_thòi',\n",
       "   'đau_khổ',\n",
       "   'vì',\n",
       "   'bọn',\n",
       "   'bán_nước',\n",
       "   'nên',\n",
       "   'không',\n",
       "   'tìm',\n",
       "   'đến',\n",
       "   'với',\n",
       "   'khoa_học',\n",
       "   'được',\n",
       "   '.',\n",
       "   '',\n",
       "   'Thương',\n",
       "   'biết_mấy',\n",
       "   'những',\n",
       "   'Thuận',\n",
       "   ',',\n",
       "   'những',\n",
       "   'Liên',\n",
       "   ',',\n",
       "   'những',\n",
       "   'Luận',\n",
       "   ',',\n",
       "   'Xuân',\n",
       "   ',',\n",
       "   'Nghĩa',\n",
       "   'mỗi',\n",
       "   'người',\n",
       "   'một',\n",
       "   'hoàn_cảnh',\n",
       "   'nhưng',\n",
       "   'đều',\n",
       "   'rất',\n",
       "   'giống',\n",
       "   'nhau',\n",
       "   ':',\n",
       "   'rất',\n",
       "   'ham',\n",
       "   'học',\n",
       "   ',',\n",
       "   'rất',\n",
       "   'cố_gắng',\n",
       "   'để',\n",
       "   'đạt',\n",
       "   'mức',\n",
       "   'hiểu_biết',\n",
       "   'cao',\n",
       "   'nhất',\n",
       "   '.',\n",
       "   '',\n",
       "   '“',\n",
       "   'Tôi',\n",
       "   'xin',\n",
       "   'cám_ơn',\n",
       "   'thượng_sĩ',\n",
       "   'Nguyễn',\n",
       "   'Trung',\n",
       "   'Hiếu',\n",
       "   '(',\n",
       "   'người',\n",
       "   'phiên_dịch',\n",
       "   'tiếng',\n",
       "   'Anh',\n",
       "   'cho',\n",
       "   'đơn_vị',\n",
       "   'tình_báo',\n",
       "   'quân_sự',\n",
       "   'số',\n",
       "   '635',\n",
       "   'của',\n",
       "   'quân_đội',\n",
       "   'Mỹ',\n",
       "   'biên_chế',\n",
       "   'bên',\n",
       "   'cạnh',\n",
       "   'lữ_đoàn',\n",
       "   'bộ_binh',\n",
       "   'số',\n",
       "   '11',\n",
       "   ',',\n",
       "   'sư_đoàn',\n",
       "   'bộ_binh',\n",
       "   '23',\n",
       "   ')',\n",
       "   ',',\n",
       "   'người',\n",
       "   'đã',\n",
       "   'cứu',\n",
       "   'cuốn',\n",
       "   'nhật_ký',\n",
       "   'của',\n",
       "   'chị',\n",
       "   'tôi',\n",
       "   'khỏi',\n",
       "   'bị',\n",
       "   'quẳng',\n",
       "   'vào',\n",
       "   'đống',\n",
       "   'lửa',\n",
       "   'bởi',\n",
       "   'anh',\n",
       "   'đã',\n",
       "   'nhận',\n",
       "   'ra',\n",
       "   'trong',\n",
       "   'cuốn',\n",
       "   'sổ',\n",
       "   'này',\n",
       "   'đã',\n",
       "   'chứa_đựng',\n",
       "   'lửa',\n",
       "   'rồi',\n",
       "   'để',\n",
       "   'anh',\n",
       "   'trao',\n",
       "   'lại',\n",
       "   'nó',\n",
       "   'cho',\n",
       "   'Fred',\n",
       "   'như',\n",
       "   'một',\n",
       "   'lời',\n",
       "   'uỷ_thác',\n",
       "   'từ',\n",
       "   'chị',\n",
       "   'tôi',\n",
       "   '.',\n",
       "   '',\n",
       "   'Anh',\n",
       "   'Hiếu',\n",
       "   'ơi',\n",
       "   ',',\n",
       "   'nay',\n",
       "   'anh',\n",
       "   'ở',\n",
       "   'đâu',\n",
       "   '?',\n",
       "   '',\n",
       "   'Nếu',\n",
       "   'đọc',\n",
       "   'được',\n",
       "   'những',\n",
       "   'dòng',\n",
       "   'này',\n",
       "   'xin',\n",
       "   'anh',\n",
       "   'hãy',\n",
       "   'lên_tiếng',\n",
       "   ',',\n",
       "   'cho',\n",
       "   'chúng_tôi',\n",
       "   'được',\n",
       "   'nhìn',\n",
       "   'vào',\n",
       "   'mắt',\n",
       "   'anh',\n",
       "   ',',\n",
       "   'để',\n",
       "   'nói',\n",
       "   'với',\n",
       "   'nhau',\n",
       "   'rằng',\n",
       "   'tiếng_nói',\n",
       "   'của',\n",
       "   'tình_yêu',\n",
       "   'và',\n",
       "   'khát_vọng',\n",
       "   'hoà_bình',\n",
       "   'có_thể',\n",
       "   'vượt',\n",
       "   'qua',\n",
       "   'mọi',\n",
       "   'chiến_tuyến',\n",
       "   'để',\n",
       "   'đến',\n",
       "   'với',\n",
       "   'những',\n",
       "   'trái_tim',\n",
       "   'tốt_lành',\n",
       "   '”',\n",
       "   '.',\n",
       "   '',\n",
       "   '25.7.68',\n",
       "   '',\n",
       "   'Một',\n",
       "   'buổi',\n",
       "   'ngồi',\n",
       "   'bên',\n",
       "   'giường_bệnh',\n",
       "   'của',\n",
       "   'Lâm',\n",
       "   '.',\n",
       "   '',\n",
       "   'Lâm',\n",
       "   'bị',\n",
       "   'một',\n",
       "   'mảnh',\n",
       "   'đạn',\n",
       "   'cối',\n",
       "   'cá_nhân',\n",
       "   'xuyên',\n",
       "   'vào',\n",
       "   'tuỷ_sống',\n",
       "   ',',\n",
       "   'mảnh',\n",
       "   'đạn',\n",
       "   'ác_nghiệt',\n",
       "   'đã',\n",
       "   'giết',\n",
       "   'chết',\n",
       "   'một',\n",
       "   'nửa',\n",
       "   'người',\n",
       "   'Lâm',\n",
       "   '-',\n",
       "   'từ',\n",
       "   'nửa',\n",
       "   'ngực',\n",
       "   'trở_xuống',\n",
       "   '.',\n",
       "   '',\n",
       "   'Lâm',\n",
       "   'hoàn_toàn',\n",
       "   'bại_liệt',\n",
       "   ',',\n",
       "   'lở_loét',\n",
       "   'và',\n",
       "   'biết_bao_nhiêu',\n",
       "   'đau_đớn',\n",
       "   'hành_hạ',\n",
       "   'Lâm',\n",
       "   '.',\n",
       "   '',\n",
       "   'Lâm',\n",
       "   'năm',\n",
       "   'nay',\n",
       "   'hai_tư',\n",
       "   'tuổi',\n",
       "   ',',\n",
       "   'là',\n",
       "   'một',\n",
       "   'cán_bộ',\n",
       "   'y_tế',\n",
       "   'xuất_sắc',\n",
       "   'của',\n",
       "   'Phổ_Văn',\n",
       "   '.',\n",
       "   '',\n",
       "   'Ban',\n",
       "   'dân_y',\n",
       "   'huyện',\n",
       "   'mới',\n",
       "   'rút',\n",
       "   'Lâm',\n",
       "   'về',\n",
       "   'bổ_sung',\n",
       "   'chưa',\n",
       "   'được',\n",
       "   'một',\n",
       "   'tháng',\n",
       "   ',',\n",
       "   'trong',\n",
       "   'lần',\n",
       "   'đi',\n",
       "   'công_tác',\n",
       "   'vừa_qua',\n",
       "   'địch',\n",
       "   'càn',\n",
       "   'đến',\n",
       "   ',',\n",
       "   'Lâm',\n",
       "   'xuống',\n",
       "   'công_sự',\n",
       "   'nhưng',\n",
       "   'khi',\n",
       "   'tay',\n",
       "   'anh',\n",
       "   'vừa',\n",
       "   'mở',\n",
       "   'nắp',\n",
       "   'công_sự',\n",
       "   'thì',\n",
       "   'bọn',\n",
       "   'Mỹ',\n",
       "   'đã',\n",
       "   'đến',\n",
       "   'sát',\n",
       "   'sau',\n",
       "   'lưng',\n",
       "   ',',\n",
       "   'một',\n",
       "   'mảnh',\n",
       "   'đạn',\n",
       "   'nhỏ',\n",
       "   'đã',\n",
       "   'giết_hại',\n",
       "   'đời',\n",
       "   'Lâm',\n",
       "   'một_cách',\n",
       "   'đau_đớn',\n",
       "   '.',\n",
       "   '',\n",
       "   'Lâm',\n",
       "   'chưa',\n",
       "   'chết',\n",
       "   'nhưng',\n",
       "   'chỉ',\n",
       "   'còn',\n",
       "   'nằm',\n",
       "   'để',\n",
       "   'chờ',\n",
       "   'chết',\n",
       "   '.',\n",
       "   '',\n",
       "   'Đứt',\n",
       "   'tuỷ_sống',\n",
       "   'trong',\n",
       "   'điều_kiện',\n",
       "   'ở',\n",
       "   'miền',\n",
       "   'Bắc',\n",
       "   'còn',\n",
       "   'bó_tay',\n",
       "   'nữa_là',\n",
       "   'ở',\n",
       "   'đây',\n",
       "   '.',\n",
       "   '',\n",
       "   'Lâm',\n",
       "   'biết',\n",
       "   'điều',\n",
       "   'đó',\n",
       "   'nên',\n",
       "   'đau_khổ',\n",
       "   'vô_cùng',\n",
       "   '.',\n",
       "   '',\n",
       "   'Chiều',\n",
       "   'nay',\n",
       "   'ngồi',\n",
       "   'bên',\n",
       "   'Lâm',\n",
       "   ',',\n",
       "   'Lâm',\n",
       "   'đưa',\n",
       "   'lá',\n",
       "   'thư',\n",
       "   'của',\n",
       "   'Hạnh',\n",
       "   '(',\n",
       "   'người',\n",
       "   'vợ',\n",
       "   'trẻ',\n",
       "   'của',\n",
       "   'Lâm',\n",
       "   ')',\n",
       "   'cho',\n",
       "   'mình',\n",
       "   'coi',\n",
       "   'rồi',\n",
       "   'nói',\n",
       "   'khẽ',\n",
       "   'với',\n",
       "   'mình',\n",
       "   ':',\n",
       "   '“',\n",
       "   'Chị',\n",
       "   'ơi',\n",
       "   ',',\n",
       "   'các',\n",
       "   'chị',\n",
       "   'tận_tình',\n",
       "   ',',\n",
       "   'gia_đình',\n",
       "   'tận_tình',\n",
       "   'nuôi',\n",
       "   'em',\n",
       "   'để',\n",
       "   'làm',\n",
       "   'gì',\n",
       "   '?',\n",
       "   '',\n",
       "   'Trước_sau',\n",
       "   'em',\n",
       "   'cũng',\n",
       "   'chết',\n",
       "   ',',\n",
       "   'em',\n",
       "   'có',\n",
       "   'sống',\n",
       "   'cũng',\n",
       "   'chỉ',\n",
       "   'làm',\n",
       "   'khổ',\n",
       "   'các',\n",
       "   'chị',\n",
       "   'và',\n",
       "   'gia_đình',\n",
       "   'mà',\n",
       "   'thôi',\n",
       "   '”',\n",
       "   '.',\n",
       "   '',\n",
       "   'Một',\n",
       "   'giọt',\n",
       "   'nước_mắt',\n",
       "   'lăn',\n",
       "   'dài',\n",
       "   'trên',\n",
       "   'gò_má',\n",
       "   'gầy',\n",
       "   'ốm',\n",
       "   'của',\n",
       "   'Lâm',\n",
       "   '.',\n",
       "   '',\n",
       "   'Thương',\n",
       "   'Lâm',\n",
       "   'vô_cùng',\n",
       "   'mà',\n",
       "   'chẳng',\n",
       "   'biết',\n",
       "   'nói',\n",
       "   'sao',\n",
       "   '.',\n",
       "   '',\n",
       "   'Nếu',\n",
       "   'đặt',\n",
       "   'mình',\n",
       "   'vào',\n",
       "   'hoàn_cảnh',\n",
       "   'của',\n",
       "   'Lâm',\n",
       "   'chắc',\n",
       "   'mình',\n",
       "   'cũng',\n",
       "   'nói',\n",
       "   'như',\n",
       "   'vậy',\n",
       "   'mà',\n",
       "   'thôi',\n",
       "   '.',\n",
       "   '',\n",
       "   'Nhưng',\n",
       "   'không_lẽ',\n",
       "   'không',\n",
       "   'động_viên',\n",
       "   'Lâm',\n",
       "   '...',\n",
       "   '',\n",
       "   'Ôi',\n",
       "   '!',\n",
       "   '',\n",
       "   'Chiến_tranh',\n",
       "   '!',\n",
       "   '',\n",
       "   'Sao',\n",
       "   'mà',\n",
       "   'đáng',\n",
       "   'căm_thù',\n",
       "   'đến',\n",
       "   'vậy',\n",
       "   'và',\n",
       "   'đáng',\n",
       "   'căm_thù',\n",
       "   'vô_cùng',\n",
       "   'là',\n",
       "   'bọn',\n",
       "   'quỉ',\n",
       "   'hiếu_chiến',\n",
       "   '.',\n",
       "   '',\n",
       "   'Vì_sao',\n",
       "   'chúng',\n",
       "   'lại',\n",
       "   'thích',\n",
       "   'đi',\n",
       "   'tàn_sát',\n",
       "   'bắn',\n",
       "   'giết',\n",
       "   'những',\n",
       "   'người',\n",
       "   'dân',\n",
       "   'hiền_lành',\n",
       "   ',',\n",
       "   'giản_dị',\n",
       "   'như',\n",
       "   'chúng_ta',\n",
       "   '?',\n",
       "   '',\n",
       "   'Vì_sao',\n",
       "   'chúng',\n",
       "   'đang_tâm',\n",
       "   'giết',\n",
       "   'chết',\n",
       "   'những',\n",
       "   'thanh_niên',\n",
       "   'còn',\n",
       "   'đang',\n",
       "   'tha_thiết',\n",
       "   'yêu_đời',\n",
       "   ',',\n",
       "   'đang',\n",
       "   'sống',\n",
       "   'và',\n",
       "   'chiến_đấu',\n",
       "   'với',\n",
       "   'bao',\n",
       "   'mơ_ước',\n",
       "   'như',\n",
       "   'Lâm',\n",
       "   ',',\n",
       "   'như',\n",
       "   'Lý',\n",
       "   ',',\n",
       "   'như',\n",
       "   'Hùng',\n",
       "   'và',\n",
       "   'nghìn',\n",
       "   'vạn',\n",
       "   'người',\n",
       "   ...]],\n",
       " [['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'I-PER',\n",
       "   'I-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-MISC',\n",
       "   'I-MISC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'O',\n",
       "   'B-ORG',\n",
       "   'I-ORG',\n",
       "   'I-ORG',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   '',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   '',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-LOC',\n",
       "   'I-LOC',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   '',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PER',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   ...]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_conll_file('/home/phamson/transformers/examples/token-classification/vlsp16/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42be3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_ner.py --data_dir=data/vlsp16 --bert_model='/home/phamson/Desktop/phobert-base-135' --task_name=ner --output_dir=out_base --max_seq_length=128 --do_train --num_train_epochs 5 --do_eval --warmup_proportion=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b84378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3746088c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3af9ae0bf7a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove ignored index (special tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m true_predictions = [\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5adc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(address=\"http://127.0.0.1\", port=9000) \n",
    "\n",
    "# Input \n",
    "text = \"Bán các lô biệt thự đẳng cấp dự án Louis City Hoàng Mai - khu đô thị gần phố cổ tại Hà Nội. \\\n",
    "Diện tích đất: 270m2 - 290m2, lô góc 300m2 - 310m2. \\\n",
    "Diện tích xây dựng: 65%. \\\n",
    "Xây 4 tầng 1 tum, 1 hầm để xe. \\\n",
    "Hướng chính: Đông, Tây, Nam, Bắc. \\\n",
    "Vị trí lô góc, 2 mặt tiền và một mặt view công viên trung tâm vườn hoa. \\\n",
    "Đường vào 40m, đường 22,5m và mặt đường 13,5m. \\\n",
    "Thiết kế theo phong cách tân cổ điển, phong cách kiến trúc kiểu Ý. \\\n",
    "Vị trí mặt đường Tân Mai đi lại vô cùng thuận tiện. \\\n",
    "Gần 2 hồ lớn: Hồ Đền Lừ, hồ Yên Sở và công viên Yên Sở. \\\n",
    "Có đầy đủ các tiện ích cao cấp: Trường học quốc tế liên cấp 1 - 2 - 3, trường mầm non, trường trung học cơ sở, trung tâm thương mại, bãi đỗ xe, công viên, bể bơi trong nhà... \\\n",
    "Giá đất: 89 triệu/m2. \\\n",
    "Giá xây dựng: 7,2 triệu/m2. \\\n",
    "Chính sách bán hàng: Chiết khấu 7%, vay ngân hàng 0% trong 18 tháng.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77087876",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = annotator.ner(text)\n",
    "result = []\n",
    "for sentence in ner:\n",
    "    entity = ''\n",
    "    for key, value in sentence:\n",
    "#         if value == \"O\":\n",
    "#             continue\n",
    "#         else if va:\n",
    "#             entity = key\n",
    "#             entity += key\n",
    "#         result.append(key)\n",
    "        if value.startswith('B'):\n",
    "            entity = key\n",
    "            continue\n",
    "        if value.startswith('I') and entity != '':\n",
    "            entity += key\n",
    "        if entity != '' and entity not in result:\n",
    "            result.append(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c4887ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hà_Nội',\n",
       " 'Nam',\n",
       " 'Bắc',\n",
       " 'đườngTân_Mai',\n",
       " 'HồĐền_Lừ',\n",
       " 'hồYên_Sở',\n",
       " 'công_viênYên_Sở']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['B-LOC', 'I-LOC']\n",
    "\n",
    "for sentence in ner:\n",
    "    for key, value in sentence:\n",
    "        myDictionary.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99e39a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Bán', 'O'),\n",
       "  ('các', 'O'),\n",
       "  ('lô', 'O'),\n",
       "  ('biệt_thự', 'O'),\n",
       "  ('đẳng_cấp', 'O'),\n",
       "  ('dự_án', 'O'),\n",
       "  ('Louis_City_Hoàng_Mai', 'O'),\n",
       "  ('-', 'O'),\n",
       "  ('khu', 'O'),\n",
       "  ('đô_thị', 'O'),\n",
       "  ('gần', 'O'),\n",
       "  ('phố', 'O'),\n",
       "  ('cổ', 'O'),\n",
       "  ('tại', 'O'),\n",
       "  ('Hà_Nội', 'B-LOC'),\n",
       "  ('.', 'O')],\n",
       " [('Diện_tích', 'O'),\n",
       "  ('đất', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('270m2', 'O'),\n",
       "  ('-', 'O'),\n",
       "  ('290m2', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('lô', 'O'),\n",
       "  ('góc', 'O'),\n",
       "  ('300m2', 'O'),\n",
       "  ('-', 'O'),\n",
       "  ('310', 'O'),\n",
       "  ('m2', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Diện_tích', 'O'), ('xây_dựng', 'O'), (':', 'O')],\n",
       " [('65%', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Xây', 'O'),\n",
       "  ('4', 'O'),\n",
       "  ('tầng', 'O'),\n",
       "  ('1', 'O'),\n",
       "  ('tum', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('1', 'O'),\n",
       "  ('hầm', 'O'),\n",
       "  ('để', 'O'),\n",
       "  ('xe', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Hướng', 'O'), ('chính', 'O'), (':', 'O')],\n",
       " [('Đông', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('Tây', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('Nam', 'B-LOC'),\n",
       "  (',', 'O'),\n",
       "  ('Bắc', 'B-LOC'),\n",
       "  ('.', 'O')],\n",
       " [('Vị_trí', 'O'),\n",
       "  ('lô', 'O'),\n",
       "  ('góc', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('2', 'O'),\n",
       "  ('mặt_tiền', 'O'),\n",
       "  ('và', 'O'),\n",
       "  ('một_mặt', 'O'),\n",
       "  ('view', 'O'),\n",
       "  ('công_viên', 'O'),\n",
       "  ('trung_tâm', 'O'),\n",
       "  ('vườn', 'O'),\n",
       "  ('hoa', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Đường', 'O'),\n",
       "  ('vào', 'O'),\n",
       "  ('40m', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('đường', 'O'),\n",
       "  ('22,5', 'O'),\n",
       "  ('m', 'O'),\n",
       "  ('và', 'O'),\n",
       "  ('mặt', 'O'),\n",
       "  ('đường', 'O'),\n",
       "  ('13,5', 'O'),\n",
       "  ('m', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Thiết_kế', 'O'),\n",
       "  ('theo', 'O'),\n",
       "  ('phong_cách', 'O'),\n",
       "  ('tân_cổ_điển', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('phong_cách', 'O'),\n",
       "  ('kiến_trúc', 'O'),\n",
       "  ('kiểu', 'O'),\n",
       "  ('Ý.', 'O'),\n",
       "  ('Vị_trí', 'O'),\n",
       "  ('mặt', 'O'),\n",
       "  ('đường', 'B-LOC'),\n",
       "  ('Tân_Mai', 'I-LOC'),\n",
       "  ('đi_lại', 'O'),\n",
       "  ('vô_cùng', 'O'),\n",
       "  ('thuận_tiện', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Gần', 'O'),\n",
       "  ('2', 'O'),\n",
       "  ('hồ', 'O'),\n",
       "  ('lớn', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('Hồ', 'B-LOC'),\n",
       "  ('Đền_Lừ', 'I-LOC'),\n",
       "  (',', 'O'),\n",
       "  ('hồ', 'B-LOC'),\n",
       "  ('Yên_Sở', 'I-LOC'),\n",
       "  ('và', 'O'),\n",
       "  ('công_viên', 'B-LOC'),\n",
       "  ('Yên_Sở', 'I-LOC'),\n",
       "  ('.', 'O')],\n",
       " [('Có', 'O'),\n",
       "  ('đầy_đủ', 'O'),\n",
       "  ('các', 'O'),\n",
       "  ('tiện_ích', 'O'),\n",
       "  ('cao_cấp', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('Trường_học', 'O'),\n",
       "  ('quốc_tế', 'O'),\n",
       "  ('liên', 'O'),\n",
       "  ('cấp', 'O'),\n",
       "  ('1', 'O'),\n",
       "  ('-', 'O'),\n",
       "  ('2', 'O'),\n",
       "  ('-', 'O'),\n",
       "  ('3', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('trường', 'O'),\n",
       "  ('mầm_non', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('trường', 'O'),\n",
       "  ('trung_học_cơ_sở', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('trung_tâm', 'O'),\n",
       "  ('thương_mại', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('bãi', 'O'),\n",
       "  ('đỗ', 'O'),\n",
       "  ('xe', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('công_viên', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('bể_bơi', 'O'),\n",
       "  ('trong', 'O'),\n",
       "  ('nhà', 'O'),\n",
       "  ('...', 'O')],\n",
       " [('Giá', 'O'),\n",
       "  ('đất', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('89', 'O'),\n",
       "  ('triệu', 'O'),\n",
       "  ('/', 'O'),\n",
       "  ('m2', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Giá', 'O'),\n",
       "  ('xây_dựng', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('7,2', 'O'),\n",
       "  ('triệu', 'O'),\n",
       "  ('/', 'O'),\n",
       "  ('m2', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('Chính_sách', 'O'),\n",
       "  ('bán', 'O'),\n",
       "  ('hàng', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('Chiết_khấu', 'O'),\n",
       "  ('7%', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('vay', 'O'),\n",
       "  ('ngân_hàng', 'O'),\n",
       "  ('0%', 'O'),\n",
       "  ('trong', 'O'),\n",
       "  ('18', 'O'),\n",
       "  ('tháng', 'O'),\n",
       "  ('.', 'O')]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ce47ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['đườngTân_Mai', 'HồĐền_Lừ', 'hồYên_Sở', 'công_viênYên_Sở']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6059d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Ông', 'O'),\n",
       "  ('Nguyễn_Khắc_Chúc', 'B-PER'),\n",
       "  ('đang', 'O'),\n",
       "  ('làm_việc', 'O'),\n",
       "  ('tại', 'O'),\n",
       "  ('Đại_học', 'B-ORG'),\n",
       "  ('Quốc_gia', 'I-ORG'),\n",
       "  ('Hà_Nội', 'I-ORG'),\n",
       "  ('.', 'O')],\n",
       " [('Bà', 'O'),\n",
       "  ('Lan', 'B-PER'),\n",
       "  (',', 'O'),\n",
       "  ('vợ', 'O'),\n",
       "  ('ông', 'O'),\n",
       "  ('Chúc', 'B-PER'),\n",
       "  (',', 'O'),\n",
       "  ('cũng', 'O'),\n",
       "  ('làm_việc', 'O'),\n",
       "  ('tại', 'O'),\n",
       "  ('đây', 'O'),\n",
       "  ('.', 'O')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator.ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "218d494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing: [['VTV', 'đồng_ý', 'chia_sẻ', 'bản_quyền', 'World_Cup', '2018', 'cho', 'HTV', 'để', 'khai_thác', '.'], ['Nhưng', 'cả', 'hai', 'nhà', 'đài', 'đều', 'phải', 'chờ', 'sự', 'đồng_ý', 'của', 'FIFA', 'mới', 'thực_hiện', 'được', 'điều', 'này', '.']]\n",
      "POS Tagging: [[('VTV', 'Ny'), ('đồng_ý', 'V'), ('chia_sẻ', 'V'), ('bản_quyền', 'N'), ('World_Cup', 'N'), ('2018', 'M'), ('cho', 'E'), ('HTV', 'Ny'), ('để', 'E'), ('khai_thác', 'V'), ('.', 'CH')], [('Nhưng', 'C'), ('cả', 'P'), ('hai', 'M'), ('nhà', 'N'), ('đài', 'N'), ('đều', 'R'), ('phải', 'V'), ('chờ', 'V'), ('sự', 'Nc'), ('đồng_ý', 'V'), ('của', 'E'), ('FIFA', 'Np'), ('mới', 'R'), ('thực_hiện', 'V'), ('được', 'R'), ('điều', 'N'), ('này', 'P'), ('.', 'CH')]]\n",
      "Named-Entity Recognizing: [[('VTV', 'B-ORG'), ('đồng_ý', 'O'), ('chia_sẻ', 'O'), ('bản_quyền', 'O'), ('World_Cup', 'O'), ('2018', 'O'), ('cho', 'O'), ('HTV', 'O'), ('để', 'O'), ('khai_thác', 'O'), ('.', 'O')], [('Nhưng', 'O'), ('cả', 'O'), ('hai', 'O'), ('nhà', 'O'), ('đài', 'O'), ('đều', 'O'), ('phải', 'O'), ('chờ', 'O'), ('sự', 'O'), ('đồng_ý', 'O'), ('của', 'O'), ('FIFA', 'B-ORG'), ('mới', 'O'), ('thực_hiện', 'O'), ('được', 'O'), ('điều', 'O'), ('này', 'O'), ('.', 'O')]]\n",
      "Dependency Parsing: [[('sub', 2, 1), ('root', 0, 2), ('vmod', 2, 3), ('dob', 3, 4), ('nmod', 4, 5), ('det', 5, 6), ('iob', 3, 7), ('pob', 7, 8), ('prp', 3, 9), ('vmod', 9, 10), ('punct', 2, 11)], [('dep', 7, 1), ('nmod', 4, 2), ('det', 4, 3), ('sub', 7, 4), ('nmod', 4, 5), ('adv', 7, 6), ('root', 0, 7), ('vmod', 7, 8), ('dob', 8, 9), ('nmod', 9, 10), ('nmod', 9, 11), ('pob', 11, 12), ('adv', 14, 13), ('vmod', 7, 14), ('adv', 14, 15), ('dob', 14, 16), ('det', 16, 17), ('punct', 7, 18)]]\n",
      "Annotating: {'sentences': [[{'index': 1, 'form': 'VTV', 'posTag': 'Ny', 'nerLabel': 'B-ORG', 'head': 2, 'depLabel': 'sub'}, {'index': 2, 'form': 'đồng_ý', 'posTag': 'V', 'nerLabel': 'O', 'head': 0, 'depLabel': 'root'}, {'index': 3, 'form': 'chia_sẻ', 'posTag': 'V', 'nerLabel': 'O', 'head': 2, 'depLabel': 'vmod'}, {'index': 4, 'form': 'bản_quyền', 'posTag': 'N', 'nerLabel': 'O', 'head': 3, 'depLabel': 'dob'}, {'index': 5, 'form': 'World_Cup', 'posTag': 'N', 'nerLabel': 'O', 'head': 4, 'depLabel': 'nmod'}, {'index': 6, 'form': '2018', 'posTag': 'M', 'nerLabel': 'O', 'head': 5, 'depLabel': 'det'}, {'index': 7, 'form': 'cho', 'posTag': 'E', 'nerLabel': 'O', 'head': 3, 'depLabel': 'iob'}, {'index': 8, 'form': 'HTV', 'posTag': 'Ny', 'nerLabel': 'O', 'head': 7, 'depLabel': 'pob'}, {'index': 9, 'form': 'để', 'posTag': 'E', 'nerLabel': 'O', 'head': 3, 'depLabel': 'prp'}, {'index': 10, 'form': 'khai_thác', 'posTag': 'V', 'nerLabel': 'O', 'head': 9, 'depLabel': 'vmod'}, {'index': 11, 'form': '.', 'posTag': 'CH', 'nerLabel': 'O', 'head': 2, 'depLabel': 'punct'}], [{'index': 1, 'form': 'Nhưng', 'posTag': 'C', 'nerLabel': 'O', 'head': 7, 'depLabel': 'dep'}, {'index': 2, 'form': 'cả', 'posTag': 'P', 'nerLabel': 'O', 'head': 4, 'depLabel': 'nmod'}, {'index': 3, 'form': 'hai', 'posTag': 'M', 'nerLabel': 'O', 'head': 4, 'depLabel': 'det'}, {'index': 4, 'form': 'nhà', 'posTag': 'N', 'nerLabel': 'O', 'head': 7, 'depLabel': 'sub'}, {'index': 5, 'form': 'đài', 'posTag': 'N', 'nerLabel': 'O', 'head': 4, 'depLabel': 'nmod'}, {'index': 6, 'form': 'đều', 'posTag': 'R', 'nerLabel': 'O', 'head': 7, 'depLabel': 'adv'}, {'index': 7, 'form': 'phải', 'posTag': 'V', 'nerLabel': 'O', 'head': 0, 'depLabel': 'root'}, {'index': 8, 'form': 'chờ', 'posTag': 'V', 'nerLabel': 'O', 'head': 7, 'depLabel': 'vmod'}, {'index': 9, 'form': 'sự', 'posTag': 'Nc', 'nerLabel': 'O', 'head': 8, 'depLabel': 'dob'}, {'index': 10, 'form': 'đồng_ý', 'posTag': 'V', 'nerLabel': 'O', 'head': 9, 'depLabel': 'nmod'}, {'index': 11, 'form': 'của', 'posTag': 'E', 'nerLabel': 'O', 'head': 9, 'depLabel': 'nmod'}, {'index': 12, 'form': 'FIFA', 'posTag': 'Np', 'nerLabel': 'B-ORG', 'head': 11, 'depLabel': 'pob'}, {'index': 13, 'form': 'mới', 'posTag': 'R', 'nerLabel': 'O', 'head': 14, 'depLabel': 'adv'}, {'index': 14, 'form': 'thực_hiện', 'posTag': 'V', 'nerLabel': 'O', 'head': 7, 'depLabel': 'vmod'}, {'index': 15, 'form': 'được', 'posTag': 'R', 'nerLabel': 'O', 'head': 14, 'depLabel': 'adv'}, {'index': 16, 'form': 'điều', 'posTag': 'N', 'nerLabel': 'O', 'head': 14, 'depLabel': 'dob'}, {'index': 17, 'form': 'này', 'posTag': 'P', 'nerLabel': 'O', 'head': 16, 'depLabel': 'det'}, {'index': 18, 'form': '.', 'posTag': 'CH', 'nerLabel': 'O', 'head': 7, 'depLabel': 'punct'}]]}\n",
      "Language: vi\n",
      "Tokenizing: [['VTV', 'đồng_ý', 'chia_sẻ', 'bản_quyền', 'World_Cup', '2018', 'cho', 'HTV', 'để', 'khai_thác', '.'], ['Nhưng', 'cả', 'hai', 'nhà', 'đài', 'đều', 'phải', 'chờ', 'sự', 'đồng_ý', 'của', 'FIFA', 'mới', 'thực_hiện', 'được', 'điều', 'này', '.']]\n",
      "POS Tagging: [[('VTV', 'Ny'), ('đồng_ý', 'V'), ('chia_sẻ', 'V'), ('bản_quyền', 'N'), ('World_Cup', 'N'), ('2018', 'M'), ('cho', 'E'), ('HTV', 'Ny'), ('để', 'E'), ('khai_thác', 'V'), ('.', 'CH')], [('Nhưng', 'C'), ('cả', 'P'), ('hai', 'M'), ('nhà', 'N'), ('đài', 'N'), ('đều', 'R'), ('phải', 'V'), ('chờ', 'V'), ('sự', 'Nc'), ('đồng_ý', 'V'), ('của', 'E'), ('FIFA', 'Np'), ('mới', 'R'), ('thực_hiện', 'V'), ('được', 'R'), ('điều', 'N'), ('này', 'P'), ('.', 'CH')]]\n",
      "Named-Entity Recognizing: [[('VTV', 'B-ORG'), ('đồng_ý', 'O'), ('chia_sẻ', 'O'), ('bản_quyền', 'O'), ('World_Cup', 'O'), ('2018', 'O'), ('cho', 'O'), ('HTV', 'O'), ('để', 'O'), ('khai_thác', 'O'), ('.', 'O')], [('Nhưng', 'O'), ('cả', 'O'), ('hai', 'O'), ('nhà', 'O'), ('đài', 'O'), ('đều', 'O'), ('phải', 'O'), ('chờ', 'O'), ('sự', 'O'), ('đồng_ý', 'O'), ('của', 'O'), ('FIFA', 'B-ORG'), ('mới', 'O'), ('thực_hiện', 'O'), ('được', 'O'), ('điều', 'O'), ('này', 'O'), ('.', 'O')]]\n",
      "Dependency Parsing: [[('sub', 2, 1), ('root', 0, 2), ('vmod', 2, 3), ('dob', 3, 4), ('nmod', 4, 5), ('det', 5, 6), ('iob', 3, 7), ('pob', 7, 8), ('prp', 3, 9), ('vmod', 9, 10), ('punct', 2, 11)], [('dep', 7, 1), ('nmod', 4, 2), ('det', 4, 3), ('sub', 7, 4), ('nmod', 4, 5), ('adv', 7, 6), ('root', 0, 7), ('vmod', 7, 8), ('dob', 8, 9), ('nmod', 9, 10), ('nmod', 9, 11), ('pob', 11, 12), ('adv', 14, 13), ('vmod', 7, 14), ('adv', 14, 15), ('dob', 14, 16), ('det', 16, 17), ('punct', 7, 18)]]\n",
      "Annotating: {'sentences': [[{'index': 1, 'form': 'VTV', 'posTag': 'Ny', 'nerLabel': 'B-ORG', 'head': 2, 'depLabel': 'sub'}, {'index': 2, 'form': 'đồng_ý', 'posTag': 'V', 'nerLabel': 'O', 'head': 0, 'depLabel': 'root'}, {'index': 3, 'form': 'chia_sẻ', 'posTag': 'V', 'nerLabel': 'O', 'head': 2, 'depLabel': 'vmod'}, {'index': 4, 'form': 'bản_quyền', 'posTag': 'N', 'nerLabel': 'O', 'head': 3, 'depLabel': 'dob'}, {'index': 5, 'form': 'World_Cup', 'posTag': 'N', 'nerLabel': 'O', 'head': 4, 'depLabel': 'nmod'}, {'index': 6, 'form': '2018', 'posTag': 'M', 'nerLabel': 'O', 'head': 5, 'depLabel': 'det'}, {'index': 7, 'form': 'cho', 'posTag': 'E', 'nerLabel': 'O', 'head': 3, 'depLabel': 'iob'}, {'index': 8, 'form': 'HTV', 'posTag': 'Ny', 'nerLabel': 'O', 'head': 7, 'depLabel': 'pob'}, {'index': 9, 'form': 'để', 'posTag': 'E', 'nerLabel': 'O', 'head': 3, 'depLabel': 'prp'}, {'index': 10, 'form': 'khai_thác', 'posTag': 'V', 'nerLabel': 'O', 'head': 9, 'depLabel': 'vmod'}, {'index': 11, 'form': '.', 'posTag': 'CH', 'nerLabel': 'O', 'head': 2, 'depLabel': 'punct'}], [{'index': 1, 'form': 'Nhưng', 'posTag': 'C', 'nerLabel': 'O', 'head': 7, 'depLabel': 'dep'}, {'index': 2, 'form': 'cả', 'posTag': 'P', 'nerLabel': 'O', 'head': 4, 'depLabel': 'nmod'}, {'index': 3, 'form': 'hai', 'posTag': 'M', 'nerLabel': 'O', 'head': 4, 'depLabel': 'det'}, {'index': 4, 'form': 'nhà', 'posTag': 'N', 'nerLabel': 'O', 'head': 7, 'depLabel': 'sub'}, {'index': 5, 'form': 'đài', 'posTag': 'N', 'nerLabel': 'O', 'head': 4, 'depLabel': 'nmod'}, {'index': 6, 'form': 'đều', 'posTag': 'R', 'nerLabel': 'O', 'head': 7, 'depLabel': 'adv'}, {'index': 7, 'form': 'phải', 'posTag': 'V', 'nerLabel': 'O', 'head': 0, 'depLabel': 'root'}, {'index': 8, 'form': 'chờ', 'posTag': 'V', 'nerLabel': 'O', 'head': 7, 'depLabel': 'vmod'}, {'index': 9, 'form': 'sự', 'posTag': 'Nc', 'nerLabel': 'O', 'head': 8, 'depLabel': 'dob'}, {'index': 10, 'form': 'đồng_ý', 'posTag': 'V', 'nerLabel': 'O', 'head': 9, 'depLabel': 'nmod'}, {'index': 11, 'form': 'của', 'posTag': 'E', 'nerLabel': 'O', 'head': 9, 'depLabel': 'nmod'}, {'index': 12, 'form': 'FIFA', 'posTag': 'Np', 'nerLabel': 'B-ORG', 'head': 11, 'depLabel': 'pob'}, {'index': 13, 'form': 'mới', 'posTag': 'R', 'nerLabel': 'O', 'head': 14, 'depLabel': 'adv'}, {'index': 14, 'form': 'thực_hiện', 'posTag': 'V', 'nerLabel': 'O', 'head': 7, 'depLabel': 'vmod'}, {'index': 15, 'form': 'được', 'posTag': 'R', 'nerLabel': 'O', 'head': 14, 'depLabel': 'adv'}, {'index': 16, 'form': 'điều', 'posTag': 'N', 'nerLabel': 'O', 'head': 14, 'depLabel': 'dob'}, {'index': 17, 'form': 'này', 'posTag': 'P', 'nerLabel': 'O', 'head': 16, 'depLabel': 'det'}, {'index': 18, 'form': '.', 'posTag': 'CH', 'nerLabel': 'O', 'head': 7, 'depLabel': 'punct'}]]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: vi\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "\n",
    "def simple_usage():\n",
    "    # Uncomment this line for debugging\n",
    "    # logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "    vncorenlp_file = r'/home/phamson/VnCoreNLP/VnCoreNLP-1.1.1.jar'\n",
    "\n",
    "    sentences = 'VTV đồng ý chia sẻ bản quyền World Cup 2018 cho HTV để khai thác. ' \\\n",
    "                'Nhưng cả hai nhà đài đều phải chờ sự đồng ý của FIFA mới thực hiện được điều này.'\n",
    "\n",
    "    # Use \"with ... as\" to close the server automatically\n",
    "    with VnCoreNLP(vncorenlp_file) as vncorenlp:\n",
    "        print('Tokenizing:', vncorenlp.tokenize(sentences))\n",
    "        print('POS Tagging:', vncorenlp.pos_tag(sentences))\n",
    "        print('Named-Entity Recognizing:', vncorenlp.ner(sentences))\n",
    "        print('Dependency Parsing:', vncorenlp.dep_parse(sentences))\n",
    "        print('Annotating:', vncorenlp.annotate(sentences))\n",
    "        print('Language:', vncorenlp.detect_language(sentences))\n",
    "\n",
    "    # In this way, you have to close the server manually by calling close function\n",
    "    vncorenlp = VnCoreNLP(vncorenlp_file)\n",
    "\n",
    "    print('Tokenizing:', vncorenlp.tokenize(sentences))\n",
    "    print('POS Tagging:', vncorenlp.pos_tag(sentences))\n",
    "    print('Named-Entity Recognizing:', vncorenlp.ner(sentences))\n",
    "    print('Dependency Parsing:', vncorenlp.dep_parse(sentences))\n",
    "    print('Annotating:', vncorenlp.annotate(sentences))\n",
    "    print('Language:', vncorenlp.detect_language(sentences))\n",
    "\n",
    "    # Do not forget to close the server\n",
    "    vncorenlp.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    simple_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ac81b46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please ensure that the annotators \"wseg,pos,ner\" are being used on the server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a8ec38231199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mannotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/vncorenlp/vncorenlp.py\u001b[0m in \u001b[0;36mner\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wseg,pos,ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'form'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nerLabel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/vncorenlp/vncorenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             assert self.annotators.issuperset(annotators.split(\n\u001b[0m\u001b[1;32m    120\u001b[0m                 ',')), 'Please ensure that the annotators \"%s\" are being used on the server.' % annotators\n\u001b[1;32m    121\u001b[0m         data = {\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please ensure that the annotators \"wseg,pos,ner\" are being used on the server."
     ]
    }
   ],
   "source": [
    "annotator.ner(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8990533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [[{'index': 1, 'form': 'Ông', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 2, 'form': 'Nguyễn_Khắc_Chúc', 'nerLabel': 'B-PER', 'head': -1},\n",
       "   {'index': 3, 'form': 'đang', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 4, 'form': 'làm_việc', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 5, 'form': 'tại', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 6, 'form': 'Đại_học', 'nerLabel': 'B-ORG', 'head': -1},\n",
       "   {'index': 7, 'form': 'Quốc_gia', 'nerLabel': 'I-ORG', 'head': -1},\n",
       "   {'index': 8, 'form': 'Hà_Nội', 'nerLabel': 'I-ORG', 'head': -1},\n",
       "   {'index': 9, 'form': '.', 'nerLabel': 'O', 'head': -1}],\n",
       "  [{'index': 1, 'form': 'Bà', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 2, 'form': 'Lan', 'nerLabel': 'B-PER', 'head': -1},\n",
       "   {'index': 3, 'form': ',', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 4, 'form': 'vợ', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 5, 'form': 'ông', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 6, 'form': 'Chúc', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 7, 'form': ',', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 8, 'form': 'cũng', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 9, 'form': 'làm_việc', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 10, 'form': 'tại', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 11, 'form': 'đây', 'nerLabel': 'O', 'head': -1},\n",
       "   {'index': 12, 'form': '.', 'nerLabel': 'O', 'head': -1}]]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d143199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', datat_files={'train':['']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "877bfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63506d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "file ='file:///home/phamson/transformers/examples/token-classification/vlsp16/dev.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb90e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x7feacbf4ef20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.reader(file, quotechar=\"'\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(file, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41c6cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-4ab6779b43ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msubword_len_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = sys.argv[1]\n",
    "model_name_or_path = sys.argv[2]\n",
    "max_len = int(sys.argv[3])\n",
    "\n",
    "subword_len_counter = 0\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "max_len -= tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "with open(dataset, \"rt\") as f_p:\n",
    "    for line in f_p:\n",
    "        line = line.rstrip()\n",
    "\n",
    "        if not line:\n",
    "            print(line)\n",
    "            subword_len_counter = 0\n",
    "            continue\n",
    "\n",
    "        token = line.split()[0]\n",
    "\n",
    "        current_subwords_len = len(tokenizer.tokenize(token))\n",
    "\n",
    "        # Token contains strange control characters like \\x96 or \\x95\n",
    "        # Just filter out the complete line\n",
    "        if current_subwords_len == 0:\n",
    "            continue\n",
    "\n",
    "        if (subword_len_counter + current_subwords_len) > max_len:\n",
    "            print(\"\")\n",
    "            print(line)\n",
    "            subword_len_counter = current_subwords_len\n",
    "            continue\n",
    "\n",
    "        subword_len_counter += current_subwords_len\n",
    "\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f9b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut: the delimiter must be a single character\n",
      "Try 'cut --help' for more information.\n",
      "cat: write error: Broken pipe\n",
      "cut: the delimiter must be a single character\n",
      "Try 'cut --help' for more information.\n",
      "cat: write error: Broken pipe\n",
      "cut: the delimiter must be a single character\n",
      "Try 'cut --help' for more information.\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# cat '/home/phamson/data/train.txt' | cut -f 1,4 > train.txt\n",
    "# cat '/home/phamson/data/test.txt' | cut -f 1,4 > test.txt\n",
    "# cat '/home/phamson/data/dev.txt' | cut -f 1,4 > dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041559eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff13700d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b0bfa6f8e8895292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/phamson/.cache/huggingface/datasets/csv/default-b0bfa6f8e8895292/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/phamson/.cache/huggingface/datasets/csv/default-b0bfa6f8e8895292/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "External features info don't match the dataset:\nGot\n{'Chị': Value(dtype='string', id=None), 'O': Value(dtype='string', id=None)}\nwith type\nstruct<Chị: string, O: string>\n\nbut expected something like\n{'Đó': Value(dtype='string', id=None), 'O': Value(dtype='string', id=None)}\nwith type\nstruct<O: string, Đó: string>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-733f12a7d69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'/home/phamson/data/test/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'/home/phamson/data/test/test.txt'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Build dataset for splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;31m# Create a dataset for each of the given splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         datasets = utils.map_nested(\n\u001b[0m\u001b[1;32m    739\u001b[0m             partial(\n\u001b[1;32m    740\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_single_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mnum_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         mapped = [\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0m_single_map_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         ]\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         mapped = [\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0m_single_map_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         ]\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# Singleton first to spare some computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;31m# Build base dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         ds = self._as_dataset(\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         )\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources_paths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arrow_table, data_files, info, split, indices_table, indices_data_files, fingerprint, inplace_history)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fingerprint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fingerprint can't be None in a Dataset object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \"External features info don't match the dataset:\\nGot\\n{}\\nwith type\\n{}\\n\\nbut expected something like\\n{}\\nwith type\\n{}\".format(\n\u001b[1;32m    286\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: External features info don't match the dataset:\nGot\n{'Chị': Value(dtype='string', id=None), 'O': Value(dtype='string', id=None)}\nwith type\nstruct<Chị: string, O: string>\n\nbut expected something like\n{'Đó': Value(dtype='string', id=None), 'O': Value(dtype='string', id=None)}\nwith type\nstruct<O: string, Đó: string>"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', delimiter='\\t', data_files={'train': '/home/phamson/data/test/train.txt','test': '/home/phamson/data/test/test.txt'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb13735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "texts, tags = read_wnut('/home/phamson/data/test/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe3d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ấn_Độ_Dương', 'sang', 'Thái_Bình_Dương', ',', 'chiếm', 'đến', 'lượng']\n",
      "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96799489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
