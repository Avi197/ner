{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebaa5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471b8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, tags_train = read_wnut('/home/phamson/data/test/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c70259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_test, tags_test = read_wnut('/home/phamson/data/test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we’ve read the data in, let’s create a train/validation split:\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665ab51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let’s create encodings for our tokens and tags. For the tags, we can start by just create a simple mapping which we’ll use in a moment:\n",
    "\n",
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26266f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<!-- To encode the tokens, we’ll use a pre-trained DistilBert tokenizer. We can tell the tokenizer that we’re dealing with ready-split tokens rather than full sentence strings by passing is_split_into_words=True. We’ll also pass padding=True and truncation=True to pad the sequences to be the same length. Lastly, we can tell the model to return information about the tokens which are split by the wordpiece tokenization process, which we will need in a moment. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346ea78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46077535",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e582f398618e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2271\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2272\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2454\u001b[0m         )\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2457\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             raise NotImplementedError(\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0;34m\"return_offset_mapping is not available when using Python tokenizers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;34m\"To use this feature, change your tokenizer to one deriving from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast."
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(texts_train, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "test_encodings = tokenizer(texts_test, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb906c7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/phamson/data/test/train.txt\n",
    "/home/phamson/data/test/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "######\n",
    "dataset = '/home/phamson/data/test/train.txt'\n",
    "model_name_or_path = 'vinai/phobert-base'\n",
    "max_len = 128\n",
    "######\n",
    "\n",
    "\n",
    "subword_len_counter = 0\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "max_len -= tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "with open(dataset, \"rt\") as f_p:\n",
    "    for line in f_p:\n",
    "        line = line.rstrip()\n",
    "\n",
    "        if not line:\n",
    "            print(line)\n",
    "            subword_len_counter = 0\n",
    "            continue\n",
    "\n",
    "        token = line.split()[0]\n",
    "\n",
    "        current_subwords_len = len(tokenizer.tokenize(token))\n",
    "\n",
    "        # Token contains strange control characters like \\x96 or \\x95\n",
    "        # Just filter out the complete line\n",
    "        if current_subwords_len == 0:\n",
    "            continue\n",
    "\n",
    "        if (subword_len_counter + current_subwords_len) > max_len:\n",
    "            print(\"\")\n",
    "            print(line)\n",
    "            subword_len_counter = current_subwords_len\n",
    "            continue\n",
    "\n",
    "        subword_len_counter += current_subwords_len\n",
    "\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28caf035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 16:33:17--  https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 991 [text/plain]\n",
      "Saving to: ‘preprocess.py’\n",
      "\n",
      "preprocess.py       100%[===================>]     991  --.-KB/s    in 0s      \n",
      "\n",
      "2021-04-23 16:33:17 (22,3 MB/s) - ‘preprocess.py’ saved [991/991]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget \"https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# use tokenizer by phobert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe8e24c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23 16:35:10.299392: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-04-23 16:35:10.299410: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2021-04-23 16:35:22.485085: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-04-23 16:35:22.485103: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2021-04-23 16:35:30.767642: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-04-23 16:35:30.767682: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "! python preprocess.py /home/phamson/data/test/train.txt $BERT_MODEL $MAX_LENGTH > /home/phamson/data/test/train_.txt\n",
    "! python preprocess.py /home/phamson/data/test/test.txt $BERT_MODEL $MAX_LENGTH > /home/phamson/data/test/test_.txt\n",
    "! python preprocess.py /home/phamson/data/test/dev.txt $BERT_MODEL $MAX_LENGTH > /home/phamson/data/test/dev_.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bdb20ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPUT_DIR=roberta\n",
      "env: BATCH_SIZE=32\n",
      "env: NUM_EPOCHS=2\n",
      "env: SAVE_STEPS=750\n",
      "env: SEED=1\n"
     ]
    }
   ],
   "source": [
    "%env OUTPUT_DIR=roberta\n",
    "%env BATCH_SIZE=32\n",
    "%env NUM_EPOCHS=2\n",
    "%env SAVE_STEPS=750\n",
    "%env SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82ebaf12",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-3700890d762d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-3700890d762d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    --model_type roberta \\\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "! python /kaggle/working/transformers/examples/ner/run_ner.py \n",
    "--data_dir /kaggle/working/JNLPBA/ \\\n",
    "--model_type roberta \\\n",
    "--labels /kaggle/working/JNLPBA/labels.txt \\\n",
    "--model_name_or_path $BERT_MODEL \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--max_seq_length  $MAX_LENGTH \\\n",
    "--num_train_epochs $NUM_EPOCHS \\\n",
    "--per_gpu_train_batch_size $BATCH_SIZE \\\n",
    "--save_steps $SAVE_STEPS \\\n",
    "--seed $SEED \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--overwrite_output_dir \\\n",
    "--evaluate_during_training \\\n",
    "--logging_steps 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "051d2db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MAX_LENGTH=128\n",
      "env: BERT_MODEL=vinai/phobert-base\n"
     ]
    }
   ],
   "source": [
    "%env MAX_LENGTH=128\n",
    "%env BERT_MODEL=vinai/phobert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74212499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPUT_DIR=phobert\n",
      "env: BATCH_SIZE=32\n",
      "env: NUM_EPOCHS=3\n",
      "env: SAVE_STEPS=750\n",
      "env: SEED=1\n"
     ]
    }
   ],
   "source": [
    "%env OUTPUT_DIR=phobert\n",
    "%env BATCH_SIZE=32\n",
    "%env NUM_EPOCHS=3\n",
    "%env SAVE_STEPS=750\n",
    "%env SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "export MAX_LENGTH=128\n",
    "export BERT_MODEL=vinai/phobert-base\n",
    "export OUTPUT_DIR=phobert-ner\n",
    "export BATCH_SIZE=32\n",
    "export NUM_EPOCHS=3\n",
    "export SAVE_STEPS=750\n",
    "export SEED=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c08cbdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-34472a24dd65>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-34472a24dd65>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    --model_name_or_path 'vinai/phobert-base' \\\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python3 'home/phamson/transformers/examples/legacy/token-classification/run_ner.py' \\\n",
    "--data_dir /home/phamson/data/tokenized/ \\\n",
    "--model_name_or_path 'vinai/phobert-base' \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--max_seq_length  $MAX_LENGTH \\\n",
    "--num_train_epochs $NUM_EPOCHS \\\n",
    "--per_device_train_batch_size $BATCH_SIZE \\\n",
    "--save_steps $SAVE_STEPS \\\n",
    "--seed $SEED \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48b5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
